% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gbm_fit.R
\name{gbm_fit}
\alias{gbm_fit}
\title{Fit a gradient boosted tree model}
\usage{
gbm_fit(
  prep_data,
  model_params = NULL,
  eta = 0.3,
  gamma = 2,
  max_depth = 6,
  min_child_weight = 1,
  subsample = 0.5,
  nrounds = 1000,
  plot_importance = TRUE,
  cv = FALSE,
  cv.random = FALSE,
  cv.random.iters = 3,
  cv.folds = 5,
  cv.eta = c(0.3, 0.1),
  cv.gamma = c(1),
  cv.maxdepth = c(6, 12, 20),
  cv.min_child_weight = c(1),
  cv.subsample = c(0.75, 0.5),
  cv.nrounds = c(500, 1000)
)
}
\arguments{
\item{prep_data}{Model list output from 'prep_data' function.}

\item{model_params}{Optional \code{xgboost} model parameters. Defaults to NULL.}

\item{eta}{Step size shrinkage (aka 'learning rate). Lower values imply smaller steps and higher shrinkage.}

\item{gamma}{Minimum loss required to make a partition on a leaf node. Larger values imply more conservative models}

\item{max_depth}{Maximum tree depth. Higher values imply more complex trees, but also more overfitting.}

\item{min_child_weight}{Minimum weight required for a leaf node. Higher values imply more conservative trees.}

\item{subsample}{Ratio of observations to randomly sub sample each boosting iteration. Lower values tend to help prevent overfitting.}

\item{plot_importance}{Should variable importance values be plotted? Defaults to TRUE.}

\item{cv}{Should model parameters be chosen using cross validation? Defaults to FALSE.}

\item{cv.eta}{Step size shrinkage (aka 'learning rate). Lower values imply smaller steps and higher shrinkage.}

\item{cv.gamma}{Minimum loss required to make a partition on a leaf node. Larger values imply more conservative models}

\item{cv.min_child_weight}{Minimum weight required for a leaf node. Higher values imply more conservative trees.}

\item{cv.subsample}{Ratio of observations to randomly sub sample each boosting iteration. Lower values tend to help prevent overfitting.}

\item{cv.nrounds}{Maximum number of boosting rounds. More rounds are needed for lower values of eta.}

\item{cv.max_depth}{Maximum tree depth. Higher values imply more complex trees, but also more overfitting.}
}
\description{
This is essentially a wrapper around the existing \code{xgboost} function. This function provides a number of conveniences to speed up model building,
including some help selecting reasonable parameters, evaluating fit, and examining variable importance.
Users should provide data prepared using the 'prep_data' function.
By default, \code{gbm_fit} attempts to fit a reasonable model using built-in parameters, or using cross-validation (preferred!).
Users are STRONGLY encouraged to tune the model parameters using some estimate of out-of-sample prediction.
The built-in cross-validation step greatly aids this step by testing a range of values in a tuning grid,
finding the model that minimizes the chosen loss function, then fitting using the optimal set of parameters.
}
\examples{

data("hartford_data")

# Prepping data for distance only
model_data <- prep_data(outcome = hartford_data$robbery,
                       pred_var = hartford_data[c('bar','nightclub','liquor','gas','pharmacy','restaurant')],
                       region = hartford_data$hartford,
                       gridsize = 200,
                       measure = 'distance')
          
fit1 <- gbm_fit(
  prep_data = model_data,
  eta = 0.3,
  gamma = 1,
  max_depth = 6,
  min_child_weight = 1,
  subsample = .5,
  nrounds = 1000,
  plot_importance = TRUE
  )

}
