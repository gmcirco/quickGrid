% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/lgbm_fit.R
\name{lgbm_fit}
\alias{lgbm_fit}
\title{Fit a \code{lightgbm} model}
\usage{
lgbm_fit(
  prep_data,
  model_params = NULL,
  nleaves = 5,
  mindata = 50,
  maxdepth = 7,
  lrate = 0.1,
  nrounds = 500,
  bag_frac = 1,
  bag_freq = 0,
  plot_importance = TRUE,
  cv = FALSE,
  cv.folds = 5,
  cv.nleaves = c(5, 10, 20),
  cv.mindata = c(50, 100),
  cv.maxdepth = c(7, 10, 20),
  cv.lrate = c(0.01),
  cv.nrounds = c(750)
)
}
\arguments{
\item{prep_data}{Model list output from 'prep_data' function.}

\item{model_params}{Optional \code{lightgbm} model parameters. Defaults to NULL.}

\item{nleaves}{Maximum number of leaves in one tree. Defaults to 5.}

\item{mindata}{Minimum number of observations to be considered for each split. Defaults to 50.}

\item{maxdepth}{Maximum tree depth. Defaults to 7.}

\item{lrate}{Learning rate. Controls the deepness or shallowness of each iteration. Smaller steps generally
produce lower test error, but require a larger number of iterations. Defaults to 0.01.}

\item{bag_frac}{Proportion of sample to randomly subset. Defaults to 1.}

\item{bag_freq}{How often to perform bagging. Defaults to 0.}

\item{plot_importance}{Plot variable importance values. Defaults to FALSE.}

\item{cv}{Should hyperparameters be chosen using cross validation? Defaults to FALSE.}

\item{cv.nleaves}{Vector of potential values for number of leaves in tuning grid.}

\item{cv.lrate}{Vector of potential values for learning rate in tuning grid.}

\item{cv.nrounds}{Vector of potential values for number of rounds in tuning grid.}

\item{plot}{Plot a prediction map. Defaults to TRUE.}
}
\description{
This is a wrapper around the \code{lightgbm} function. This function provides a number of conveniences to speed up model building.
Users should provide data prepared using the 'prep_data' function. 'lgmb_fit' attempts to fit a reasonable model using some default
parameters, or using cross-validation (preferred!). Users are STRONGLY encouraged to tune the model parameters using some
estimate of out-of-sample prediction. The built-in cross-validation step greatly aids this step by testing a range of values in a tuning grid,
finding the best model, then fitting using the chosen parameters.
}
