---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# quickGrid

<!-- badges: start -->
<!-- badges: end -->

`quickgrid` is a package intended to make it easy and fast to generate place-based crime predictions using a variety of spatial risk factors. Tools are included in this package to help speed up the processing of data (including calculating distance and density measures for spatial risk factors) and the fitting of models for prediction estimates. Currently, this package utilizes a gradient boosted tree model implementing the `lightgbm` package. A series of wrappers help users fit reasonable starting models using a cross-validated tuning grid. Much of the inspiration of this package comes from an earlier paper by [Wheeler and Steenbeck (2020)](https://link.springer.com/article/10.1007/s10940-020-09457-7).

## Installation

You can install the current experimental version at [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("gmcirco/quickgrid")
```

## The Major Functions

`quickGrid` has two major functions. One sets up the raw data for model fitting (`prep_data`) and the other simplifies the fitting of a lightgbm model (`fit_lgbm`).

### prep_data
The general workflow for the `quickGrid` package is to first set up your data using the `prep_data` function. This takes an outcome variable (for example, a crime type), a named list of predictor variables, and a study boundary and converts it into a format that can be fed into a statistical modeling program. This function converts your study area into $N$ gridded cells, then calculates distances or densities of various predictor variables.

### fit_lgbm

`fit_lgbm` takes the data created by `prep_data` and fits a predictive model using a tree-based boosted model via `lightgbm`. While some reasonable defaults are set for the function, it is highly reccomended that users make use of the built-in cross-validation function to help choose hyperparameters that will minimize overfitting. The `fit_lgbm` function will automatically fit either binary $(0,1)$, Poisson $(0,1,2,..n)$, or regression. You can also specify custom models using the parameters listed [here](https://lightgbm.readthedocs.io/en/latest/Parameters.html).


## Data Example: Hartford, CT Robberies

This is a minimum working example using the data provided in the `quickGrid` package^[**NOTE**: Currently a bug in the R version of `lightgbm` can cause crashes if it is loaded *after* any tidyverse packages. The only solution is to ensure you load `lightgbm` prior to doing anything else. See [here](https://github.com/microsoft/LightGBM/issues/4007) for more info. This will be fixed in future versions.]. `quickGrid` is packaged with an example dataset containing robberies in Hartford CT for 2018 and 2019. A number of spatial predictors are packed in as well, including the locations of bars, liquor stores, gas stations, pharmacies, and dollar stores (labeled 'retail'). For simplicity, these are packaged as a list of `sf` objects that can be easily plugged into our model prep function.

### Setting up your data

```{r example}
library(lightgbm)
library(quickGrid)

data("hartford_data")

names(hartford_data)
 
model_data <-
  prep_data(
    outcome = hartford_data[['robbery']],
    pred_var = hartford_data[c("bar", "liquor", "gas", "pharmacy", "retail")],
    region = hartford_data[['hartford']],
    gridsize = 200,
    measure = 'distance')
```

This will give us a list of two objects: a model dataframe with the necessary variables attached and a shapefile corresponding to the grid cells falling within the study boundaries. The model dataframe, in particular, has a few important fields:

1. X-Y coordinates and a unique grid cell identifier
2. Grid counts of the outcome variable (denoted `n`)
3. Distances or densities to the nearest predictor feature 

```{r}
# Top 6 rows of model dataframe
head(model_data$lgbm_dataframe)
```

Here we have the distances for each of the 5 predictor features for each of the 12,901 cell 200x200 foot cells. Now that it has been processed, this data can be either directly fed into the `lgbm_fit` function, or used in any other model fitting function. Here, we will use it directly in the convenient `lgbm_fit` function to fit a predictive model for robberies.

### Fitting your model

Here, we're going to fit a simple model using some parameters that were already selected via cross-validation. We will put in the model data, and specify a few parameters for our boosted tree-based model. We'll set the maximum number of leaves to 10, the learning rate to 0.01, and the number of iterations to 750. In addition, to protect against overfitting we'll set the bagging fraction to .5, and the bagging frequency to 5. For more information on parameter tuning, check the [lightgbm page](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html)

Finally, we'll set `plot = TRUE` and `plot_importance = TRUE` to give us the predicted grid values on a map and the feature importance for each predictor variable.

```{r, out.width="600px"}
gbm_fit <- lgbm_fit(prep_data = model_data,
                    nleaves = 10,
                    lrate = 0.01,
                    nrounds = 750,
                    bag_frac = .5,
                    bag_freq = 5,
                    plot = TRUE,
                    plot_importance = TRUE)
```


We can directly access the model predictions as well, by examinng the model dataframe in the `lgbm_fit` output file. In this case `gbm.pred` is the predicted number of robberies at any given grid cell. These predictions can be used in a variety of methods - including identifying hot-spots for crime prevention.

```{r}
head(gbm_fit$model_dataframe)
```
### Fitting your model with cross-validation

Another (highly-suggested) option is to utilize the cross-validation function in `lgbm_fit` to determine the optimal value for the model parameters. Currently, the cross-validation function chooses values for the number of leaves, the learning rate, and the number of iterations. These can be specified by adding `cv=TRUE` to the model, then providing values for the cross-validation tuning grid for each parameter. The cross-validation function then iteratively fits models for each of the unique parameter combinations until an optimal one is found. If we provide 3 values for each of the parameters we must then fit $3^3 = 27$ models. While more options are typically better, this can become quite time intensive with many combinations.

```{r, eval=FALSE}
gbm_fit_cv <- lgbm_fit(prep_data = model_data,
                    cv = TRUE,
                    cv.nleaves = c(5,10,20),
                    cv.lrate = c(0.1, 0.05, 0.01),
                    cv.nrounds = c(250,500,750),
                    bag_frac = .5,
                    bag_freq = 5)
```
